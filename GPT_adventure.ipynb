{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfkDTeZtE5nxGG15wRWYXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NelsenYoung/GPT_adventure/blob/main/GPT_adventure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u7k9YPX_IuI",
        "outputId": "ca41337f-0a64-4ba7-c2aa-d26e0a90baee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-05 22:30:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2026-01-05 22:30:23 (16.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup of the boilerplate\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\") as f:\n",
        "  text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "408cIMoh_rnS",
        "outputId": "3f16a849-495e-4975-f2f9-f1e12d101ebf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6yvrbPT13yY",
        "outputId": "e89fffcb-3dd7-4f92-a530-c6bffabe7bfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gnQEfDWMt4i",
        "outputId": "8be887b2-b66a-48b2-92b6-3de760d32112"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the dataset\n",
        "chars = sorted(list(set(text)))\n",
        "chars.insert(0, ']')\n",
        "stoi = {s:i for i, s in enumerate(chars)}\n",
        "itos = {i:s for i, s in enumerate(chars)}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fsG4uVbthNx5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll need to find the portion that actually takes a ton of time.\n",
        "## GUESSES\n",
        "# Encoding the block\n",
        "# Sliding window over the block\n",
        "# Blocking logic"
      ],
      "metadata": {
        "id": "3OVk4C8a-MNO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the character-wise dataset\n",
        "block_size = 8\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "# For each token, grab a block\n",
        "for i in range(len(text) - (block_size)):\n",
        "  xs = text[i:(i+block_size)]\n",
        "  ys = text[i+block_size]\n",
        "\n",
        "  # Encode the block\n",
        "  xs = [stoi[i] for i in xs]\n",
        "  ys = [stoi[i] for i in ys]\n",
        "\n",
        "  X.append(xs)\n",
        "  Y += ys\n",
        "  if i % 100000 == 0:\n",
        "    print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqkrAtPEADOY",
        "outputId": "9b613d38-1220-4607-f68c-7208004d36be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100000\n",
            "200000\n",
            "300000\n",
            "400000\n",
            "500000\n",
            "600000\n",
            "700000\n",
            "800000\n",
            "900000\n",
            "1000000\n",
            "1100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X), len(Y))\n",
        "num_examples = len(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96adEfJzQVJV",
        "outputId": "dd7fa2af-25f2-43ec-d1f6-b5d1dc4148a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115386 1115386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)"
      ],
      "metadata": {
        "id": "7if8a3uXO6H5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into test and train splits\n",
        "test_train_split = 8\n",
        "split = (num_examples // 10) * test_train_split\n",
        "X_train = X[:split]\n",
        "Y_train = Y[:split]\n",
        "X_test = X[split:]\n",
        "Y_test = Y[split:]"
      ],
      "metadata": {
        "id": "UGNwRHxFSase"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For mini batching, all we want to do is grab 32 random numbers in range 0 and train examlpes size\n",
        "batch_size = 32\n",
        "train_data_size = X_train.shape[0]\n",
        "idxs = torch.randint(train_data_size, (batch_size,))\n",
        "x = X_train[idxs]\n",
        "y = Y_train[idxs]\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P09P92r5cYY7",
        "outputId": "73788ff9-1af9-4e80-8f8d-ee76985ef7e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}